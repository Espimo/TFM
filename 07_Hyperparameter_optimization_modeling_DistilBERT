import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification
import os
import json
from datetime import datetime

# Initial configuration
batch_size = 16
epochs = 10
learning_rate = 3e-5
max_length = 128
weight_decay = 0.01
output_dir = 'job_classification_model'
checkpoint_dir = os.path.join(output_dir, 'checkpoints')
epoch_models_dir = os.path.join(output_dir, 'epoch_models')  # New directory for epoch models
os.makedirs(output_dir, exist_ok=True)
os.makedirs(checkpoint_dir, exist_ok=True)
os.makedirs(epoch_models_dir, exist_ok=True)  # Create directory for epoch models

# Logging configuration
log_file = os.path.join(output_dir, 'training_log.txt')

def log_message(message):
    """Logs a message to both console and file"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    log_entry = f"[{timestamp}] {message}"
    print(log_entry)
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(log_entry + '\n')

# Custom callback to save the model after each epoch
class SaveModelEachEpoch(tf.keras.callbacks.Callback):
    def __init__(self, save_dir, tokenizer):
        super(SaveModelEachEpoch, self).__init__()
        self.save_dir = save_dir
        self.tokenizer = tokenizer
    
    def on_epoch_end(self, epoch, logs=None):
        epoch_dir = os.path.join(self.save_dir, f'epoch_{epoch + 1}')
        os.makedirs(epoch_dir, exist_ok=True)
        self.model.save_pretrained(epoch_dir)
        self.tokenizer.save_pretrained(epoch_dir)
        metrics = {
            'loss': logs.get('loss'),
            'val_loss': logs.get('val_loss'),
            'accuracy': logs.get('accuracy'),
            'val_accuracy': logs.get('val_accuracy'),
            'precision': logs.get('precision'),
            'val_precision': logs.get('val_precision'),
            'recall': logs.get('recall'),
            'val_recall': logs.get('val_recall')
        }
        with open(os.path.join(epoch_dir, 'metrics.json'), 'w') as f:
            json.dump(metrics, f)
        log_message(f"Model and metrics saved for epoch {epoch + 1}")

# GPU configuration
tf.config.set_soft_device_placement(True)
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    try:
        for device in physical_devices:
            tf.config.experimental.set_memory_growth(device, True)
        log_message("GPU configured for dynamic memory growth")
    except RuntimeError as e:
        log_message(f"Error configuring GPU: {e}")
else:
    log_message("No GPUs detected. Training will proceed on CPU")

# Additional data loading and processing functions
def load_and_preprocess_data(min_samples_per_class=2):
    log_message("Starting data loading and preprocessing...")
    df = pd.read_csv('dataset_ready.csv', usecols=['job_summary'])
    initial_samples = len(df)
    df = df.dropna(subset=['job_summary'])
    texts = df['job_summary'].astype(str).tolist()

    log_message(f"Loaded texts: {initial_samples} initial samples, {len(texts)} after removing nulls")

    labels_df = pd.read_csv('dataset_skills_binarized.csv', dtype=np.int8)
    class_counts = labels_df.sum()
    valid_columns = class_counts[class_counts >= min_samples_per_class].index
    labels_df = labels_df[valid_columns]
    labels = labels_df.values
    valid_indices = labels.sum(axis=1) > 0
    texts = [text for idx, text in enumerate(texts) if valid_indices[idx]]
    labels = labels[valid_indices]
    texts_train, texts_test, y_train, y_test = train_test_split(
        texts,
        labels,
        test_size=0.2,
        random_state=42,
        stratify=labels_df.sum(axis=1)
    )
    log_message(f"Data split completed: {len(texts_train)} training, {len(texts_test)} test")
    return texts_train, texts_test, y_train, y_test, valid_columns.tolist()

def create_tf_dataset(texts, labels, tokenizer, is_training=True):
    encodings = tokenizer(
        texts,
        truncation=True,
        padding='max_length',
        max_length=max_length,
        return_tensors='tf'
    )
    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings), labels))
    if is_training:
        dataset = dataset.shuffle(10000, reshuffle_each_iteration=True)
    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return dataset

def compute_class_weights(y_train):
    """Calculates class weights to handle imbalance"""
    n_samples = len(y_train)
    class_counts = y_train.sum(axis=0)
    class_weights = n_samples / (len(class_counts) * class_counts)
    return class_weights

def custom_loss(class_weights):
    """Custom loss that accounts for class weights"""
    def weighted_binary_crossentropy(y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        weights = tf.constant(class_weights, dtype=tf.float32)
        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=True)
        sample_weights = tf.reduce_sum(y_true * weights, axis=1)
        return tf.reduce_mean(bce * sample_weights)
    return weighted_binary_crossentropy

def train():
    log_message("Starting training process...")

    texts_train, texts_test, y_train, y_test, label_list = load_and_preprocess_data(min_samples_per_class=2)
    num_labels = len(label_list)
    log_message(f"Number of labels: {num_labels}")

    class_weights = compute_class_weights(y_train)
    log_message("Class weights calculated to handle imbalance")

    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
    base_model = TFDistilBertForSequenceClassification.from_pretrained(
        'distilbert-base-uncased',
        num_labels=num_labels,
        problem_type="multi_label_classification"
    )

    optimizer = tf.keras.optimizers.experimental.AdamW(
        learning_rate=learning_rate,
        weight_decay=weight_decay
    )
    loss_fn = custom_loss(class_weights)
    metrics = [
        tf.keras.metrics.BinaryAccuracy(name='accuracy'),
        tf.keras.metrics.Precision(name='precision'),
        tf.keras.metrics.Recall(name='recall')
    ]

    base_model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)

    train_dataset = create_tf_dataset(texts_train, y_train, tokenizer, is_training=True)
    test_dataset = create_tf_dataset(texts_test, y_test, tokenizer, is_training=False)

    callbacks = [
        SaveModelEachEpoch(epoch_models_dir, tokenizer),
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=2,
            restore_best_weights=True,
            mode='min'
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.2,
            patience=1,
            verbose=1,
            mode='min'
        ),
        tf.keras.callbacks.ModelCheckpoint(
            filepath=os.path.join(checkpoint_dir, 'ckpt_{epoch}'),
            save_weights_only=True,
            save_best_only=True,
            monitor='val_loss',
            mode='min',
            verbose=1
        )
    ]

    log_message("Starting training...")
    base_model.fit(
        train_dataset,
        validation_data=test_dataset,
        epochs=epochs,
        callbacks=callbacks
    )

    base_model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    log_message("Model and tokenizer saved successfully")
    log_message("Training completed")

# Execute training
train()
